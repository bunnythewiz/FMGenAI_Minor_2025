\bibitem{han2024progressive}
Han, Xu and Jin, Linghao and Liu, Xiaofeng and Liang, Paul Pu.
\newblock Progressive Compositionality in Text-to-Image Generative Models.
\newblock In \emph{The Thirteenth International Conference on Learning Representations (ICLR)}, 2025.
\newblock \url{https://openreview.net/forum?id=...}

\bibitem{evogen_github}
Han, Xu and Jin, Linghao and Liu, Xiaofeng and Liang, Paul Pu.
\newblock EvoGen: Official Repository for Progressive Compositionality in Text-to-Image Generative Models.
\newblock GitHub repository, 2024.
\newblock \url{https://github.com/evansh666/EvoGen}

\bibitem{evogen_project}
Han, Xu and Jin, Linghao and Liu, Xiaofeng and Liang, Paul Pu.
\newblock EvoGen Project Page.
\newblock Project website with demos and data, 2024.
\newblock \url{https://evansh666.github.io/EvoGen_Page/}

\bibitem{stable_diffusion}
Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Björn.
\newblock High-Resolution Image Synthesis with Latent Diffusion Models.
\newblock \emph{arXiv preprint arXiv:2112.10752}, 2021.
\newblock \url{https://arxiv.org/abs/2112.10752}

\bibitem{huggingface_diffusers}
von Platen, Patrick and Patil, Suraj and Lozhkov, Anton and Cuenca, Pedro and Lambert, Nathan and Rasul, Kashif and Davaadorj, Mishig and Wolf, Thomas.
\newblock Diffusers: State-of-the-art diffusion models.
\newblock HuggingFace library, 2022.
\newblock \url{https://github.com/huggingface/diffusers}

\bibitem{clip_openai}
Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya.
\newblock Learning Transferable Visual Representations of Text.
\newblock \emph{arXiv preprint arXiv:2103.00020}, 2021.
\newblock \url{https://arxiv.org/abs/2103.00020}

\bibitem{pytorch}
Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and others.
\newblock PyTorch: An Imperative Style, High-Performance Deep Learning Library.
\newblock In \emph{Advances in Neural Information Processing Systems 32}, pages 8024--8035. Curran Associates, Inc., 2019.
\newblock \href{http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}{url}

\bibitem{transformers_huggingface}
Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Perric and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and von Platen, Patrick and Ma, Clara and Jernite, Yacine and others.
\newblock Transformers: State-of-the-Art Natural Language Processing.
\newblock \emph{arXiv preprint arXiv:1910.03771}, 2019.
\newblock \url{https://arxiv.org/abs/1910.03771}

\bibitem{igboanusi2002igbo}
Herbert Igboanusi.
\textit{Igbo English in the Nigerian Novel}.
LINCOM Europa, 2002.


\bibitem{gut2004nigerian}
Ulrike Gut.
\textit{Nigerian English: phonology}.
A handbook of varieties of English, vol. 1, pp. 813--830.
Mouton de Gruyter, 2004.


\bibitem{faraclas1996nigerian}
Nicholas G. Faraclas.
\textit{Nigerian Pidgin}.
Routledge, 1996.


\bibitem{kunchukuttan2020indicnlp}
Anoop Kunchukuttan.
The IndicNLP Library.
In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 187--194, 2020.


\bibitem{kakwani2020indicnlpsuite}
Divyanshu Kakwani, Anoop Kunchukuttan, Satish Golla, NC Gokul, Arindam Bhattacharyya, Mitesh M. Khapra, Pratyush Kumar.
IndicNLPSuite: Monolingual corpora, evaluation benchmarks and pre-trained multilingual language models for Indian languages.
\textit{arXiv preprint arXiv:2010.11418}, 2020.

\bibitem{anthropic2024claude}
Anthropic.
Claude 4: Technical Report.
\textit{arXiv preprint arXiv:2404.xxxxx}, 2024.
URL: https://www.anthropic.com/claude

\bibitem{kenton2019bert}
Jacob Devlin, Ming-Wei Chang, Kristina Toutanova.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.
\textit{arXiv preprint arXiv:1810.04805}, 2019.

\bibitem{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D. Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Saxena, Sandhini Sharma, et al.
Language models are few-shot learners.
\textit{Advances in Neural Information Processing Systems}, 33:1877–1901, 2020.

\bibitem{conneau2020unsupervised}
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov.
Unsupervised Cross-lingual Representation Learning at Scale.
In Proceedings of ACL, pp. 8440–8451, 2020.

\bibitem{liu2021lost}
Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Yonatan Belinkov, David Wingate.
Lost in the Middle: How Language Models Use Long Contexts.
\textit{arXiv preprint arXiv:2307.03172}, 2023.

\bibitem{kamper2020multilingual}
Herman Kamper, Karen Livescu.
Multilingual and cross-lingual speech emotion recognition on English and French.
In \textit{ICASSP}, pp. 7364–7368, IEEE, 2020.

\bibitem{qin2022codecswitching}
Kai Qin, Haiyang Xiong, Donghan Zhao, Jingyu Liu.
Code-switching for enhancing NMT with pre-specified translation.
\textit{arXiv preprint arXiv:2204.05869}, 2022.

\bibitem{khanuja2020gluecos}
Simran Khanuja, Sandipan Dandapat, Anirudh Srinivasan, Sunayana Sitaram, Monojit Choudhury.
GLUECoS: An evaluation benchmark for code-switched NLP.
In Proceedings of ACL, pp. 3575–3585, 2020.

\bibitem{winata2019codeswitching}
Genta Indra Winata, Andrea Madotto, Chien-Sheng Wu, Pascale Fung.
Are multilingual models effective in code-switching?
\textit{arXiv preprint arXiv:1909.07026}, 2019.

\bibitem{chakravarthi2020corpus}
Bharathi Raja Chakravarthi, Vigneshwaran Muralidaran, Ruba Priyadharshini, John Philip McCrae.
A corpus for multilingual document classification in Indian languages.
In Proceedings of LREC, pp. 6912–6919, 2020.

\bibitem{ponti2020xcopa}
Edoardo Maria Ponti, Goran Glavaš, Olga Majewska, Qianchu Liu, Ivan Vulić, Anna Korhonen.
XCOPA: A multilingual dataset for causal commonsense reasoning.
\textit{arXiv preprint arXiv:2005.00333}, 2020.

\bibitem{hu2020xtreme}
Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, Melvin Johnson.
XTREME: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation.
In \textit{ICML}, pp. 4411–4421, 2020.

\bibitem{tiktoken2023}
OpenAI.
tiktoken: Fast BPE tokeniser for use with OpenAI's models.
Version 0.5.1, 2023.
URL: https://github.com/openai/tiktoken

\bibitem{pandas2023}
Wes McKinney et al.
pandas: Powerful data structures for data analysis, time series, and statistics.
Version 2.0.0, 2023.
URL: https://pandas.pydata.org/

\bibitem{matplotlib2023}
John D. Hunter.
Matplotlib: A 2D graphics environment.
\textit{Computing in Science \& Engineering}, 9(3):90–95, 2007.

\bibitem{seaborn2023}
Michael Waskom et al.
Seaborn: Statistical data visualization.
Version 0.12.0, 2023.
URL: https://seaborn.pydata.org/

\bibitem{scipy2023}
Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, et al.
SciPy: Open source scientific tools for Python.
Version 1.10.0, 2023.
URL: https://www.scipy.org/
