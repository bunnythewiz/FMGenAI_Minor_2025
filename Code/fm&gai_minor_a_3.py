# -*- coding: utf-8 -*-
"""FM&GAI_Minor_A_3

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B3ZjNIdb76BwB0UEOBs5VRveiTWPVfiq

# 3Q) EVOGEN Minimal Reproduction Setup for Google Colab

## Prerequisites

Step 1: Colab Environment Setup
"""

# Commented out IPython magic to ensure Python compatibility.
# First cell - Check GPU and install dependencies
import torch
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}")
print(f"CUDA version: {torch.version.cuda}")

# Install required packages
# !pip install -q diffusers transformers accelerate datasets
# !pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
# !pip install -q Pillow requests tqdm
# !pip install -q git+https://github.com/openai/CLIP.git

# Clone the repository
# !git clone https://github.com/evansh666/EvoGen.git
# %cd EvoGen

"""## Step 2: Minimal Dataset Generation (Colab Cell)"""

# Second cell - Generate minimal contrastive dataset
import torch
from diffusers import StableDiffusionPipeline
import json
import os
from PIL import Image
import matplotlib.pyplot as plt

def generate_minimal_dataset():
    print("üî• Generating minimal contrastive dataset...")

    # Simple test prompts - Stage I (single object)
    test_prompts = [
        {"positive": "A red car", "negative": "A blue car", "category": "color"},
        {"positive": "Two dogs", "negative": "One dog", "category": "counting"},
        {"positive": "A round clock", "negative": "A square clock", "category": "shape"}
    ]

    # Initialize SD model (using smaller model for faster loading)
    model_id = "runwayml/stable-diffusion-v1-5"
    try:
        pipe = StableDiffusionPipeline.from_pretrained(
            model_id,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            use_safetensors=True,
            safety_checker=None,  # Disable for speed
            requires_safety_checker=False
        )

        if torch.cuda.is_available():
            pipe = pipe.to("cuda")
            pipe.enable_attention_slicing()  # Reduce VRAM usage
            pipe.enable_model_cpu_offload()  # Further VRAM optimization

    except Exception as e:
        print(f"Failed to load SD model: {e}")
        print("Trying CompVis model instead...")
        model_id = "CompVis/stable-diffusion-v1-4"
        pipe = StableDiffusionPipeline.from_pretrained(
            model_id, torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
        )
        if torch.cuda.is_available():
            pipe = pipe.to("cuda")

    os.makedirs("minimal_dataset", exist_ok=True)

    dataset = []
    generated_images = []

    for i, prompt_pair in enumerate(test_prompts):
        print(f"üì∏ Generating pair {i+1}/3: {prompt_pair['positive']} vs {prompt_pair['negative']}")

        try:
            # Generate positive image
            with torch.autocast("cuda" if torch.cuda.is_available() else "cpu"):
                pos_img = pipe(
                    prompt_pair["positive"],
                    num_inference_steps=13,
                    guidance_scale=7.5,
                    height=256,
                    width=256
                ).images[0]

            pos_path = f"minimal_dataset/pos_{i}.png"
            pos_img.save(pos_path)

            # Generate negative image
            with torch.autocast("cuda" if torch.cuda.is_available() else "cpu"):
                neg_img = pipe(
                    prompt_pair["negative"],
                    num_inference_steps=13,
                    guidance_scale=7.5,
                    height=256,
                    width=256
                ).images[0]

            neg_path = f"minimal_dataset/neg_{i}.png"
            neg_img.save(neg_path)

            dataset.append({
                "positive_prompt": prompt_pair["positive"],
                "negative_prompt": prompt_pair["negative"],
                "positive_image": pos_path,
                "negative_image": neg_path,
                "category": prompt_pair["category"]
            })

            generated_images.append((pos_img, neg_img, prompt_pair))

        except Exception as e:
            print(f"‚ùå Error generating images for pair {i}: {e}")
            continue

    # Save metadata
    with open("minimal_dataset/metadata.json", "w") as f:
        json.dump(dataset, f, indent=2)

    print(f"‚úÖ Generated {len(dataset)} contrastive pairs!")

    # Display generated images
    fig, axes = plt.subplots(len(generated_images), 2, figsize=(10, 4*len(generated_images)))
    if len(generated_images) == 1:
        axes = axes.reshape(1, -1)

    for i, (pos_img, neg_img, prompt_pair) in enumerate(generated_images):
        axes[i, 0].imshow(pos_img)
        axes[i, 0].set_title(f"Positive: {prompt_pair['positive']}")
        axes[i, 0].axis('off')

        axes[i, 1].imshow(neg_img)
        axes[i, 1].set_title(f"Negative: {prompt_pair['negative']}")
        axes[i, 1].axis('off')

    plt.tight_layout()
    plt.show()

    return dataset

# Run dataset generation
dataset = generate_minimal_dataset()

"""## Step 3: Minimal Contrastive Training (Colab Cell)"""

# Third cell - Simplified contrastive training implementation
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from transformers import CLIPTextModel, CLIPTokenizer
import json
from PIL import Image
import numpy as np
from torchvision import transforms
import matplotlib.pyplot as plt

class MinimalContrastiveDataset(Dataset):
    def __init__(self, metadata_path, transform=None):
        with open(metadata_path, 'r') as f:
            self.data = json.load(f)
        self.transform = transform

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]

        # Load images
        pos_img = Image.open(item['positive_image']).convert('RGB')
        neg_img = Image.open(item['negative_image']).convert('RGB')

        if self.transform:
            pos_img = self.transform(pos_img)
            neg_img = self.transform(neg_img)

        return {
            'positive_image': pos_img,
            'negative_image': neg_img,
            'positive_prompt': item['positive_prompt'],
            'negative_prompt': item['negative_prompt'],
            'category': item['category']
        }

class ContrastiveLoss(nn.Module):
    def __init__(self, temperature=0.1):
        super().__init__()
        self.temperature = temperature

    def forward(self, pos_features, neg_features, text_features):
        # Normalize features
        pos_features = F.normalize(pos_features, p=2, dim=1)
        neg_features = F.normalize(neg_features, p=2, dim=1)
        text_features = F.normalize(text_features, p=2, dim=1)

        # Compute similarities
        pos_sim = torch.sum(pos_features * text_features, dim=1) / self.temperature
        neg_sim = torch.sum(neg_features * text_features, dim=1) / self.temperature

        # Contrastive loss - maximize pos_sim, minimize neg_sim
        loss = -torch.log(torch.exp(pos_sim) / (torch.exp(pos_sim) + torch.exp(neg_sim)))
        return loss.mean()

# Simple feature extractor (replaces UNet feature extraction for sanity check)
class SimpleFeatureExtractor(nn.Module):
    def __init__(self, input_dim=3*256*256, hidden_dim=512, output_dim=512):
        super().__init__()
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, output_dim)
        self.dropout = nn.Dropout(0.1)

    def forward(self, x):
        x = self.flatten(x)
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x

def run_minimal_training():
    print("Setting up minimal training test...")

    # Load text encoder
    model_id = "openai/clip-vit-base-patch32"
    try:
        tokenizer = CLIPTokenizer.from_pretrained(model_id)
        text_encoder = CLIPTextModel.from_pretrained(model_id)
    except:
        print("Using alternative CLIP model...")
        model_id = "openai/clip-vit-base-patch16"
        tokenizer = CLIPTokenizer.from_pretrained(model_id)
        text_encoder = CLIPTextModel.from_pretrained(model_id)

    device = "cuda" if torch.cuda.is_available() else "cpu"
    text_encoder = text_encoder.to(device)

    # Create simple feature extractor (proxy for UNet features)
    feature_extractor = SimpleFeatureExtractor().to(device)

    # Setup data
    transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.ToTensor(),
        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    ])

    try:
        dataset = MinimalContrastiveDataset("minimal_dataset/metadata.json", transform)
        print(f"Dataset loaded with {len(dataset)} samples")
    except Exception as e:
        print(f"Error loading dataset: {e}")
        return

    dataloader = DataLoader(dataset, batch_size=1, shuffle=True)

    # Setup training
    optimizer = torch.optim.AdamW(feature_extractor.parameters(), lr=1e-4)
    contrastive_loss_fn = ContrastiveLoss()

    print("Starting minimal training loop...")
    losses = []

    for epoch in range(3):  # 3 epochs for sanity check
        epoch_losses = []

        for batch_idx, batch in enumerate(dataloader):
            optimizer.zero_grad()

            # Move to device
            pos_images = batch['positive_image'].to(device)
            neg_images = batch['negative_image'].to(device)

            # Encode text (positive prompt)
            pos_tokens = tokenizer(
                batch['positive_prompt'],
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=77
            ).to(device)

            with torch.no_grad():
                pos_text_embeds = text_encoder(pos_tokens.input_ids)[0]
                pos_text_features = pos_text_embeds.mean(dim=1)  # Average pooling

            # Extract image features
            pos_img_features = feature_extractor(pos_images)
            neg_img_features = feature_extractor(neg_images)

            # Compute contrastive loss
            loss = contrastive_loss_fn(pos_img_features, neg_img_features, pos_text_features)

            loss.backward()
            torch.nn.utils.clip_grad_norm_(feature_extractor.parameters(), 1.0)
            optimizer.step()

            epoch_losses.append(loss.item())

            print(f"Epoch {epoch+1}, Batch {batch_idx+1}: Loss = {loss.item():.4f}, Category = {batch['category'][0]}")

        avg_loss = np.mean(epoch_losses)
        losses.append(avg_loss)
        print(f"Epoch {epoch+1} Average Loss: {avg_loss:.4f}")

    print("Training completed!")

    # Plot loss curve
    plt.figure(figsize=(8, 5))
    plt.plot(range(1, len(losses)+1), losses, 'bo-')
    plt.title('Training Loss Curve')
    plt.xlabel('Epoch')
    plt.ylabel('Contrastive Loss')
    plt.grid(True)
    plt.show()

    return losses

# Run minimal training
losses = run_minimal_training()

"""## Step 4: Verification and Results (Colab Cell)"""

# Fourth cell - Verify dataset and results
import os
import json
from PIL import Image
import matplotlib.pyplot as plt

def verify_results():
    print("üîç Verifying generated dataset and training results...")

    # Check dataset structure
    if os.path.exists("minimal_dataset"):
        files = os.listdir("minimal_dataset")
        print(f"‚úÖ Dataset folder contains: {files}")

        # Load and verify metadata
        try:
            with open("minimal_dataset/metadata.json", "r") as f:
                data = json.load(f)
            print(f"‚úÖ Dataset size: {len(data)} contrastive pairs")

            for i, item in enumerate(data):
                print(f"   Pair {i+1}: {item['category']} - '{item['positive_prompt']}' vs '{item['negative_prompt']}'")

                # Verify images exist
                if os.path.exists(item['positive_image']) and os.path.exists(item['negative_image']):
                    pos_img = Image.open(item['positive_image'])
                    neg_img = Image.open(item['negative_image'])
                    print(f"     ‚úÖ Images: {pos_img.size} and {neg_img.size}")
                else:
                    print(f"     ‚ùå Missing image files")

        except Exception as e:
            print(f"‚ùå Error reading metadata: {e}")
    else:
        print("‚ùå Dataset folder not found")

    # Check if training completed successfully
    if 'losses' in globals() and losses:
        print(f"\n‚úÖ Training completed with final loss: {losses[-1]:.4f}")
        print(f"   Loss reduction: {losses[0]:.4f} ‚Üí {losses[-1]:.4f} ({((losses[0] - losses[-1])/losses[0]*100):+.1f}%)")

        if losses[-1] < losses[0]:
            print("   ‚úÖ Loss decreased - contrastive learning working!")
        else:
            print("   ‚ö†Ô∏è  Loss increased - may need more training or different parameters")
    else:
        print("‚ùå Training results not found")

    print("\nüéØ Sanity check summary:")
    dataset_ok = os.path.exists("minimal_dataset/metadata.json")
    training_ok = 'losses' in globals() and losses and losses[-1] < losses[0] * 1.5

    print(f"   Dataset generation: {'‚úÖ PASS' if dataset_ok else '‚ùå FAIL'}")
    print(f"   Training loop: {'‚úÖ PASS' if training_ok else '‚ùå FAIL'}")

    if dataset_ok and training_ok:
        print("   üéâ EVOGEN minimal reproduction successful!")
    else:
        print("   üîß Issues detected - check error messages above")

# Run verification
verify_results()

"""## Step 5: Complete Colab Notebook (Run All Cells)"""

# Final comprehensive cell - Complete EVOGEN reproduction
import torch
import torch.nn.functional as F
from transformers import CLIPProcessor, CLIPModel
import numpy as np
import matplotlib.pyplot as plt

def evaluate_compositional_understanding():
    """
    Evaluate if the model learned compositional differences
    by measuring feature similarities
    """
    print("Evaluating compositional understanding...")

    # Load CLIP for evaluation
    clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
    clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

    device = "cuda" if torch.cuda.is_available() else "cpu"
    clip_model = clip_model.to(device)

    # Load our dataset
    with open("minimal_dataset/metadata.json", "r") as f:
        data = json.load(f)

    results = {}

    for item in data:
        category = item['category']
        pos_prompt = item['positive_prompt']
        neg_prompt = item['negative_prompt']

        # Load images
        pos_img = Image.open(item['positive_image'])
        neg_img = Image.open(item['negative_image'])

        # Process with CLIP
        inputs = clip_processor(
            text=[pos_prompt, neg_prompt],
            images=[pos_img, neg_img],
            return_tensors="pt",
            padding=True
        ).to(device)

        with torch.no_grad():
            outputs = clip_model(**inputs)
            image_embeds = outputs.image_embeds
            text_embeds = outputs.text_embeds

        # Compute similarities
        pos_img_pos_text = F.cosine_similarity(image_embeds[0:1], text_embeds[0:1])
        pos_img_neg_text = F.cosine_similarity(image_embeds[0:1], text_embeds[1:2])
        neg_img_pos_text = F.cosine_similarity(image_embeds[1:2], text_embeds[0:1])
        neg_img_neg_text = F.cosine_similarity(image_embeds[1:2], text_embeds[1:2])

        # Alignment score (higher = better compositional understanding)
        alignment_score = (pos_img_pos_text.item() + neg_img_neg_text.item()) - \
                         (pos_img_neg_text.item() + neg_img_pos_text.item())

        results[category] = {
            'positive_alignment': pos_img_pos_text.item(),
            'negative_alignment': neg_img_neg_text.item(),
            'cross_alignment_1': pos_img_neg_text.item(),
            'cross_alignment_2': neg_img_pos_text.item(),
            'compositional_score': alignment_score
        }

        print(f"{category.upper()} Category:")
        print(f"  Positive image-text similarity: {pos_img_pos_text.item():.3f}")
        print(f"  Negative image-text similarity: {neg_img_neg_text.item():.3f}")
        print(f"  Compositional score: {alignment_score:.3f}")

    return results

def create_summary_report(losses, eval_results):
    """Create a comprehensive summary of the reproduction results"""

    print("\n" + "="*50)
    print("EVOGEN MINIMAL REPRODUCTION SUMMARY")
    print("="*50)

    # Training results
    if losses:
        initial_loss = losses[0]
        final_loss = losses[-1]
        improvement = (initial_loss - final_loss) / initial_loss * 100

        print(f"\nTRAINING RESULTS:")
        print(f"  Initial loss: {initial_loss:.4f}")
        print(f"  Final loss: {final_loss:.4f}")
        print(f"  Improvement: {improvement:+.1f}%")
        print(f"  Status: {'PASS' if improvement > 0 else 'NEEDS TUNING'}")

    # Compositional evaluation
    if eval_results:
        print(f"\nCOMPOSITIONAL EVALUATION:")
        avg_score = np.mean([r['compositional_score'] for r in eval_results.values()])
        print(f"  Average compositional score: {avg_score:.3f}")
        print(f"  Status: {'PASS' if avg_score > 0 else 'NEEDS IMPROVEMENT'}")

        for category, metrics in eval_results.items():
            score = metrics['compositional_score']
            status = "GOOD" if score > 0.1 else "WEAK" if score > 0 else "POOR"
            print(f"    {category}: {score:.3f} ({status})")

    # Overall assessment
    training_ok = losses and (losses[-1] < losses[0])
    compositional_ok = eval_results and avg_score > 0

    print(f"\nOVERALL STATUS:")
    print(f"  Training convergence: {'PASS' if training_ok else 'FAIL'}")
    print(f"  Compositional learning: {'PASS' if compositional_ok else 'FAIL'}")

    if training_ok and compositional_ok:
        print(f"\nüéâ EVOGEN REPRODUCTION SUCCESSFUL!")
        print("The model demonstrates:")
        print("- Stable contrastive training")
        print("- Basic compositional understanding")
        print("- Ability to distinguish attribute differences")
    else:
        print(f"\n‚ö†Ô∏è PARTIAL SUCCESS - AREAS FOR IMPROVEMENT:")
        if not training_ok:
            print("- Training instability (try lower learning rate)")
        if not compositional_ok:
            print("- Weak compositional signals (need more diverse data)")

    print("\nNEXT STEPS FOR FULL IMPLEMENTATION:")
    print("1. Scale to 15k contrastive pairs")
    print("2. Implement 3-stage curriculum learning")
    print("3. Add VQA-based quality filtering")
    print("4. Test on T2I-CompBench benchmark")

# Run complete evaluation
try:
    eval_results = evaluate_compositional_understanding()
    create_summary_report(losses if 'losses' in globals() else None, eval_results)
except Exception as e:
    print(f"Evaluation error: {e}")
    print("Running basic verification instead...")
    verify_results()